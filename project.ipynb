{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "conv1d = tf.layers.conv1d\n",
    "\n",
    "def attn_head(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf.contrib.layers.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation\n",
    "            else:\n",
    "                ret = ret + seq\n",
    "\n",
    "    return activation(ret)  # activation\n",
    "\n",
    "# Experimental sparse attention head (for running on datasets such as Pubmed)\n",
    "# N.B. Because of limitations of current TF implementation, will work _only_ if batch_size = 1!\n",
    "def sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.layers.conv1d(seq_fts, 1, 1)\n",
    "        \n",
    "        f_1 = tf.reshape(f_1, (nb_nodes, 1))\n",
    "        f_2 = tf.reshape(f_2, (nb_nodes, 1))\n",
    "        #print(f_1.shape,adj_mat.shape)\n",
    "\n",
    "        f_1 = adj_mat*f_1\n",
    "        f_2 = adj_mat * tf.transpose(f_2, [1,0])\n",
    "\n",
    "        logits = tf.sparse_add(f_1, f_2)\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices, \n",
    "                values=tf.nn.leaky_relu(logits.values), \n",
    "                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                    values=tf.nn.dropout(coefs.values, 1.0 - coef_drop),\n",
    "                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)\n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)\n",
    "        vals = tf.expand_dims(vals, axis=0)\n",
    "        vals.set_shape([1, nb_nodes, out_sz])\n",
    "        ret = tf.contrib.layers.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation\n",
    "            else:\n",
    "                ret = ret + seq\n",
    "\n",
    "    return activation(ret) # activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.trainable_variables()\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "\n",
    "        # optimizer\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss+lossL2)\n",
    "        \n",
    "        return train_op\n",
    "\n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)\n",
    "        return tf.confusion_matrix(labels, preds)\n",
    "\n",
    "##########################\n",
    "# Adapted from tkipf/gcn #\n",
    "##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        loss=tf.reduce_mean(loss,axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from utils import layers\n",
    "#from models.base_gattn import BaseGAttN\n",
    "\n",
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "            bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                out_sz=hid_units[0], activation=activation,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                    out_sz=hid_units[i], activation=activation,\n",
    "                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(lattn_head(h_1, bias_mat=bias_mat,\n",
    "                out_sz=nb_classes, activation=lambda x: x,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "    \n",
    "        return logits\n",
    "\n",
    "#*****************************************************************************************\n",
    "\n",
    "\n",
    "class SpGAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "            bias_mat, hid_units, n_heads, activation=tf.nn.elu, \n",
    "            residual=False):\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(sp_attn_head(inputs,  \n",
    "                adj_mat=bias_mat,\n",
    "                out_sz=hid_units[0], activation=activation, nb_nodes=nb_nodes,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(sp_attn_head(h_1,  \n",
    "                    adj_mat=bias_mat,\n",
    "                    out_sz=hid_units[i], activation=activation, nb_nodes=nb_nodes,\n",
    "                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(sp_attn_head(h_1, adj_mat=bias_mat,\n",
    "                out_sz=nb_classes, activation=lambda x: x, nb_nodes=nb_nodes,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\n",
    "        \"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    print_log(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    # training nodes are training docs, no initial features\n",
    "    # print(\"x: \", x)\n",
    "    # test nodes are training docs, no initial features\n",
    "    # print(\"tx: \", tx)\n",
    "    # both labeled and unlabeled training instances are training docs and words\n",
    "    # print(\"allx: \", allx)\n",
    "    # training labels are training doc labels\n",
    "    # print(\"y: \", y)\n",
    "    # test labels are test doc labels\n",
    "    # print(\"ty: \", ty)\n",
    "    # ally are labels for labels for allx, some will not have labels, i.e., all 0\n",
    "    # print(\"ally: \\n\")\n",
    "    # for i in ally:\n",
    "    # if(sum(i) == 0):\n",
    "    # print(i)\n",
    "    # graph edge weight is the word co-occurence or doc word frequency\n",
    "    # no need to build map, directly build csr_matrix\n",
    "    # print('graph : ', graph)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(\n",
    "            min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    # print(len(labels))\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    # print(idx_test)\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_corpus(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "    print(\"SHAPE ALLX\",allx.shape,\"shape x\",tx.shape)\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    print(\"feature shape\",features.shape)\n",
    "    labels = np.vstack((ally, ty))\n",
    "    # print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"./data/{}.train.index\".format(dataset_str))\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, test_size\n",
    "\n",
    "\n",
    "def load_corpus1(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input corpus from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"./data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    # print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "    print(\"SHAPE ALLX\",allx.shape,\"shape x\",tx.shape)\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    print(\"feature shape\",features.shape)\n",
    "    labels = np.vstack((ally, ty))\n",
    "    # print(len(labels))\n",
    "\n",
    "    train_idx_orig = parse_index_file(\n",
    "        \"./data/{}.train.index\".format(dataset_str))\n",
    "    train_size = len(train_idx_orig)\n",
    "\n",
    "    val_size = train_size - x.shape[0]\n",
    "    test_size = tx.shape[0]\n",
    "\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + val_size)\n",
    "    idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "\n",
    "\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    return adj, features,labels,idx_train,idx_val,idx_test\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    # return sparse_to_tuple(features)\n",
    "    return features.A\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return adj_normalized.A\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i]\n",
    "                      for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print_log(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (\n",
    "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    # t_k.append(sp.eye(adj.shape[0]))\n",
    "    # t_k.append(scaled_laplacian)\n",
    "    t_k.append(sp.eye(adj.shape[0]).A)\n",
    "    t_k.append(scaled_laplacian.A)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    # return sparse_to_tuple(t_k)\n",
    "    return t_k\n",
    "\n",
    "\n",
    "def loadWord2Vec(filename):\n",
    "    \"\"\"Read Word Vectors\"\"\"\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    word_vector_map = {}\n",
    "    file = open(filename, 'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if(len(row) > 2):\n",
    "            vocab.append(row[0])\n",
    "            vector = row[1:]\n",
    "            length = len(vector)\n",
    "            for i in range(length):\n",
    "                vector[i] = float(vector[i])\n",
    "            embd.append(vector)\n",
    "            word_vector_map[row[0]] = vector\n",
    "    print_log('Loaded Word Vectors!')\n",
    "    file.close()\n",
    "    return vocab, embd, word_vector_map\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "import datetime\n",
    "def print_log(msg='', end='\\n'):\n",
    "    now = datetime.datetime.now()\n",
    "    t = str(now.year) + '/' + str(now.month) + '/' + str(now.day) + ' ' \\\n",
    "      + str(now.hour).zfill(2) + ':' + str(now.minute).zfill(2) + ':' + str(now.second).zfill(2)\n",
    "\n",
    "    if isinstance(msg, str):\n",
    "        lines = msg.split('\\n')\n",
    "    else:\n",
    "        lines = [msg]\n",
    "        \n",
    "    for line in lines:\n",
    "        if line == lines[-1]:\n",
    "            print('[' + t + '] ' + str(line), end=end)\n",
    "        else: \n",
    "            print('[' + t + '] ' + str(line))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when', 'of', 'for', 're', 'there', 'over', \"couldn't\", 'am', \"wouldn't\", \"she's\", 'how', 'a', 'himself', 'into', 'until', \"wasn't\", \"aren't\", 'yourselves', \"didn't\", 'between', 'm', 'ourselves', 'below', 'they', 'it', \"won't\", 'but', 'couldn', \"that'll\", 'not', 'out', \"hasn't\", 'such', 'only', 'yourself', 'he', 'both', \"mustn't\", 'all', 'was', 'weren', 'him', 'should', 'some', 'me', 'each', 'o', 'we', 'myself', 'whom', 'isn', 'yours', 'who', 'her', 'up', 'nor', 'do', 'didn', \"you'll\", 'through', 'his', 'few', 'very', 'with', 'been', 'once', 'ma', 'did', 'at', 'y', 'same', 's', \"needn't\", 'she', \"hadn't\", 'what', 'itself', 'because', 'as', 'shouldn', 'which', 'in', 'can', \"should've\", 'you', 'mustn', 'having', 'them', 'their', 'mightn', 'while', 'from', 'being', 'where', 'to', 'here', \"doesn't\", 'against', 'no', \"shan't\", 'after', 'does', \"don't\", 'hasn', \"mightn't\", \"it's\", 'most', 'too', 'now', 'these', 'on', 'hers', 'then', 'this', 't', 'will', 'so', 'aren', 'have', 'an', \"shouldn't\", 'ain', 'down', 'its', 'hadn', 'further', 'those', 'haven', \"weren't\", \"isn't\", \"you'd\", 'd', 'before', 'under', 'were', 'themselves', 'or', 'll', 'herself', 'that', 'the', 'theirs', 'shan', 'by', 'our', \"haven't\", 'be', 'just', 'other', 'i', 'ours', 'during', 'are', 'had', 'off', 'is', 'any', 'more', \"you're\", 'doing', 'and', \"you've\", 'own', 'needn', 'again', 'why', 'above', 'my', 'don', 'doesn', 'wasn', 'wouldn', 'than', 'your', 've', 'about', 'has', 'won', 'if'}\n",
      "Min_len : 14\n",
      "Max_len : 35702\n",
      "Average_len : 221.25697760798047\n",
      "{'when', 'of', 'for', 're', 'there', 'over', \"couldn't\", 'am', \"wouldn't\", \"she's\", 'how', 'a', 'himself', 'into', 'until', \"wasn't\", \"aren't\", 'yourselves', \"didn't\", 'between', 'm', 'ourselves', 'below', 'they', 'it', \"won't\", 'but', 'couldn', \"that'll\", 'not', 'out', \"hasn't\", 'such', 'only', 'yourself', 'he', 'both', \"mustn't\", 'all', 'was', 'weren', 'him', 'should', 'some', 'me', 'each', 'o', 'we', 'myself', 'whom', 'isn', 'yours', 'who', 'her', 'up', 'nor', 'do', 'didn', \"you'll\", 'through', 'his', 'few', 'very', 'with', 'been', 'once', 'ma', 'did', 'at', 'y', 'same', 's', \"needn't\", 'she', \"hadn't\", 'what', 'itself', 'because', 'as', 'shouldn', 'which', 'in', 'can', \"should've\", 'you', 'mustn', 'having', 'them', 'their', 'mightn', 'while', 'from', 'being', 'where', 'to', 'here', \"doesn't\", 'against', 'no', \"shan't\", 'after', 'does', \"don't\", 'hasn', \"mightn't\", \"it's\", 'most', 'too', 'now', 'these', 'on', 'hers', 'then', 'this', 't', 'will', 'so', 'aren', 'have', 'an', \"shouldn't\", 'ain', 'down', 'its', 'hadn', 'further', 'those', 'haven', \"weren't\", \"isn't\", \"you'd\", 'd', 'before', 'under', 'were', 'themselves', 'or', 'll', 'herself', 'that', 'the', 'theirs', 'shan', 'by', 'our', \"haven't\", 'be', 'just', 'other', 'i', 'ours', 'during', 'are', 'had', 'off', 'is', 'any', 'more', \"you're\", 'doing', 'and', \"you've\", 'own', 'needn', 'again', 'why', 'above', 'my', 'don', 'doesn', 'wasn', 'wouldn', 'than', 'your', 've', 'about', 'has', 'won', 'if'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_len : 4\n",
      "Max_len : 520\n",
      "Average_len : 65.72126661454261\n",
      "{'when', 'of', 'for', 're', 'there', 'over', \"couldn't\", 'am', \"wouldn't\", \"she's\", 'how', 'a', 'himself', 'into', 'until', \"wasn't\", \"aren't\", 'yourselves', \"didn't\", 'between', 'm', 'ourselves', 'below', 'they', 'it', \"won't\", 'but', 'couldn', \"that'll\", 'not', 'out', \"hasn't\", 'such', 'only', 'yourself', 'he', 'both', \"mustn't\", 'all', 'was', 'weren', 'him', 'should', 'some', 'me', 'each', 'o', 'we', 'myself', 'whom', 'isn', 'yours', 'who', 'her', 'up', 'nor', 'do', 'didn', \"you'll\", 'through', 'his', 'few', 'very', 'with', 'been', 'once', 'ma', 'did', 'at', 'y', 'same', 's', \"needn't\", 'she', \"hadn't\", 'what', 'itself', 'because', 'as', 'shouldn', 'which', 'in', 'can', \"should've\", 'you', 'mustn', 'having', 'them', 'their', 'mightn', 'while', 'from', 'being', 'where', 'to', 'here', \"doesn't\", 'against', 'no', \"shan't\", 'after', 'does', \"don't\", 'hasn', \"mightn't\", \"it's\", 'most', 'too', 'now', 'these', 'on', 'hers', 'then', 'this', 't', 'will', 'so', 'aren', 'have', 'an', \"shouldn't\", 'ain', 'down', 'its', 'hadn', 'further', 'those', 'haven', \"weren't\", \"isn't\", \"you'd\", 'd', 'before', 'under', 'were', 'themselves', 'or', 'll', 'herself', 'that', 'the', 'theirs', 'shan', 'by', 'our', \"haven't\", 'be', 'just', 'other', 'i', 'ours', 'during', 'are', 'had', 'off', 'is', 'any', 'more', \"you're\", 'doing', 'and', \"you've\", 'own', 'needn', 'again', 'why', 'above', 'my', 'don', 'doesn', 'wasn', 'wouldn', 'than', 'your', 've', 'about', 'has', 'won', 'if'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_len : 4\n",
      "Max_len : 612\n",
      "Average_len : 69.81989010989011\n",
      "{'when', 'of', 'for', 're', 'there', 'over', \"couldn't\", 'am', \"wouldn't\", \"she's\", 'how', 'a', 'himself', 'into', 'until', \"wasn't\", \"aren't\", 'yourselves', \"didn't\", 'between', 'm', 'ourselves', 'below', 'they', 'it', \"won't\", 'but', 'couldn', \"that'll\", 'not', 'out', \"hasn't\", 'such', 'only', 'yourself', 'he', 'both', \"mustn't\", 'all', 'was', 'weren', 'him', 'should', 'some', 'me', 'each', 'o', 'we', 'myself', 'whom', 'isn', 'yours', 'who', 'her', 'up', 'nor', 'do', 'didn', \"you'll\", 'through', 'his', 'few', 'very', 'with', 'been', 'once', 'ma', 'did', 'at', 'y', 'same', 's', \"needn't\", 'she', \"hadn't\", 'what', 'itself', 'because', 'as', 'shouldn', 'which', 'in', 'can', \"should've\", 'you', 'mustn', 'having', 'them', 'their', 'mightn', 'while', 'from', 'being', 'where', 'to', 'here', \"doesn't\", 'against', 'no', \"shan't\", 'after', 'does', \"don't\", 'hasn', \"mightn't\", \"it's\", 'most', 'too', 'now', 'these', 'on', 'hers', 'then', 'this', 't', 'will', 'so', 'aren', 'have', 'an', \"shouldn't\", 'ain', 'down', 'its', 'hadn', 'further', 'those', 'haven', \"weren't\", \"isn't\", \"you'd\", 'd', 'before', 'under', 'were', 'themselves', 'or', 'll', 'herself', 'that', 'the', 'theirs', 'shan', 'by', 'our', \"haven't\", 'be', 'just', 'other', 'i', 'ours', 'during', 'are', 'had', 'off', 'is', 'any', 'more', \"you're\", 'doing', 'and', \"you've\", 'own', 'needn', 'again', 'why', 'above', 'my', 'don', 'doesn', 'wasn', 'wouldn', 'than', 'your', 've', 'about', 'has', 'won', 'if'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_len : 14\n",
      "Max_len : 476\n",
      "Average_len : 135.8177027027027\n",
      "{'when', 'of', 'for', 're', 'there', 'over', \"couldn't\", 'am', \"wouldn't\", \"she's\", 'how', 'a', 'himself', 'into', 'until', \"wasn't\", \"aren't\", 'yourselves', \"didn't\", 'between', 'm', 'ourselves', 'below', 'they', 'it', \"won't\", 'but', 'couldn', \"that'll\", 'not', 'out', \"hasn't\", 'such', 'only', 'yourself', 'he', 'both', \"mustn't\", 'all', 'was', 'weren', 'him', 'should', 'some', 'me', 'each', 'o', 'we', 'myself', 'whom', 'isn', 'yours', 'who', 'her', 'up', 'nor', 'do', 'didn', \"you'll\", 'through', 'his', 'few', 'very', 'with', 'been', 'once', 'ma', 'did', 'at', 'y', 'same', 's', \"needn't\", 'she', \"hadn't\", 'what', 'itself', 'because', 'as', 'shouldn', 'which', 'in', 'can', \"should've\", 'you', 'mustn', 'having', 'them', 'their', 'mightn', 'while', 'from', 'being', 'where', 'to', 'here', \"doesn't\", 'against', 'no', \"shan't\", 'after', 'does', \"don't\", 'hasn', \"mightn't\", \"it's\", 'most', 'too', 'now', 'these', 'on', 'hers', 'then', 'this', 't', 'will', 'so', 'aren', 'have', 'an', \"shouldn't\", 'ain', 'down', 'its', 'hadn', 'further', 'those', 'haven', \"weren't\", \"isn't\", \"you'd\", 'd', 'before', 'under', 'were', 'themselves', 'or', 'll', 'herself', 'that', 'the', 'theirs', 'shan', 'by', 'our', \"haven't\", 'be', 'just', 'other', 'i', 'ours', 'during', 'are', 'had', 'off', 'is', 'any', 'more', \"you're\", 'doing', 'and', \"you've\", 'own', 'needn', 'again', 'why', 'above', 'my', 'don', 'doesn', 'wasn', 'wouldn', 'than', 'your', 've', 'about', 'has', 'won', 'if'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/swyam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_len : 1\n",
      "Max_len : 56\n",
      "Average_len : 20.38585631213656\n"
     ]
    }
   ],
   "source": [
    "#filter#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "#from utils.utils import clean_str, loadWord2Vec  \n",
    "#\n",
    "\n",
    "#if len(sys.argv) != 2:\n",
    "#\tsys.exit(\"Use: python remove_words.py <dataset>\")\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "for data in datasets:\n",
    "    dataset = data\n",
    "\n",
    "    if dataset not in datasets:\n",
    "        sys.exit(\"wrong dataset name\")\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(stop_words)\n",
    "\n",
    "    # Read Word Vectors\n",
    "    # word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n",
    "    # vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "    # word_embeddings_dim = len(embd[0])\n",
    "    # dataset = '20ng'\n",
    "\n",
    "    doc_content_list = []\n",
    "    #with open('data/wiki_long_abstracts_en_text.txt', 'r') as f:\n",
    "    with open('data/corpus/' + dataset + '.txt', 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip().decode('latin1'))\n",
    "\n",
    "\n",
    "    word_freq = {}  # to remove rare words\n",
    "\n",
    "    for doc_content in doc_content_list:\n",
    "        temp = clean_str(doc_content)\n",
    "        words = temp.split()\n",
    "        for word in words:\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    clean_docs = []\n",
    "    for doc_content in doc_content_list:\n",
    "        temp = clean_str(doc_content)\n",
    "        words = temp.split()\n",
    "        doc_words = []\n",
    "        for word in words:\n",
    "            # word not in stop_words and word_freq[word] >= 5\n",
    "            if dataset == 'mr':\n",
    "                doc_words.append(word)\n",
    "            elif word not in stop_words and word_freq[word] >= 5:\n",
    "                doc_words.append(word)\n",
    "\n",
    "        doc_str = ' '.join(doc_words).strip()\n",
    "        #if doc_str == '':\n",
    "            #doc_str = temp\n",
    "        clean_docs.append(doc_str)\n",
    "\n",
    "    clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "\n",
    "    #with open('../data/wiki_long_abstracts_en_text.clean.txt', 'w') as f:\n",
    "    with open('data/datax/' + dataset + '.clean.txt', 'w') as f:\n",
    "        f.write(clean_corpus_str)\n",
    "\n",
    "    #dataset = '20ng'\n",
    "    min_len = 10000\n",
    "    aver_len = 0\n",
    "    max_len = 0 \n",
    "\n",
    "    #with open('../data/wiki_long_abstracts_en_text.txt', 'r') as f:\n",
    "    with open('data/datax/' + dataset + '.clean.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            temp = line.split()\n",
    "            aver_len = aver_len + len(temp)\n",
    "            if len(temp) < min_len:\n",
    "                min_len = len(temp)\n",
    "            if len(temp) > max_len:\n",
    "                max_len = len(temp)\n",
    "\n",
    "    aver_len = 1.0 * aver_len / len(lines)\n",
    "    print('Min_len : ' + str(min_len))\n",
    "    print('Max_len : ' + str(max_len))\n",
    "    print('Average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10183, 300) (10183, 20) (7532, 300) (7532, 20) (54071, 300) (54071, 20)\n",
      "(4937, 300) (4937, 8) (2189, 300) (2189, 8) (13173, 300) (13173, 8)\n",
      "(5879, 300) (5879, 52) (2568, 300) (2568, 52) (15424, 300) (15424, 52)\n",
      "(3022, 300) (3022, 23) (4043, 300) (4043, 23) (17514, 300) (17514, 23)\n",
      "(6398, 300) (6398, 2) (3554, 300) (3554, 2) (25872, 300) (25872, 2)\n"
     ]
    }
   ],
   "source": [
    "#********************text**********************************************************\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "# import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "#from utils.utils import loadWord2Vec, clean_str\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if len(sys.argv) != 2:\n",
    "\tsys.exit(\"Use: python build_graph.py <dataset>\")\n",
    "\"\"\"\n",
    "#datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "# build corpus\n",
    "for data in  datasets:\n",
    "    dataset = data\n",
    "\n",
    "\n",
    "    if dataset not in datasets:\n",
    "        sys.exit(\"wrong dataset name\")\n",
    "\n",
    "\n",
    "    # Read Word Vectors\n",
    "    # word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
    "    # word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "    #_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "    # word_embeddings_dim = len(embd[0])\n",
    "\n",
    "    word_embeddings_dim = 300\n",
    "    word_vector_map = {}\n",
    "\n",
    "    # shulffing\n",
    "    doc_name_list = []\n",
    "    doc_train_list = []\n",
    "    doc_test_list = []\n",
    "\n",
    "    with open('data/' + dataset + '.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            doc_name_list.append(line.strip())\n",
    "            temp = line.split(\"\\t\")\n",
    "            if temp[1].find('test') != -1:\n",
    "                doc_test_list.append(line.strip())\n",
    "            elif temp[1].find('train') != -1:\n",
    "                doc_train_list.append(line.strip())\n",
    "    # print(doc_train_list)\n",
    "    # print(doc_test_list)\n",
    "\n",
    "    doc_content_list = []\n",
    "    with open('data/datax/' + dataset + '.clean.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            doc_content_list.append(line.strip())\n",
    "    # print(doc_content_list)\n",
    "\n",
    "    train_ids = []\n",
    "    for train_name in doc_train_list:\n",
    "        train_id = doc_name_list.index(train_name)\n",
    "        train_ids.append(train_id)\n",
    "    #print(train_ids)\n",
    "    random.shuffle(train_ids)\n",
    "\n",
    "    # partial labeled data\n",
    "    #train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "    train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "    with open('data/datax/' + dataset + '.train.index', 'w') as f:\n",
    "        f.write(train_ids_str)\n",
    "\n",
    "\n",
    "    test_ids = []\n",
    "    for test_name in doc_test_list:\n",
    "        test_id = doc_name_list.index(test_name)\n",
    "        test_ids.append(test_id)\n",
    "    #print(test_ids)\n",
    "    random.shuffle(test_ids)\n",
    "\n",
    "    test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "    with open('data/datax/' + dataset + '.test.index', 'w') as f:\n",
    "        f.write(test_ids_str)\n",
    "\n",
    "\n",
    "    ids = train_ids + test_ids\n",
    "    #print(ids)\n",
    "    #print(len(ids))\n",
    "\n",
    "    shuffle_doc_name_list = []\n",
    "    shuffle_doc_words_list = []\n",
    "    for id in ids:\n",
    "        shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "        shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "    shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "    shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "    with open('data/datax/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "        f.write(shuffle_doc_name_str)\n",
    "\n",
    "    with open('data/datax/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "        f.write(shuffle_doc_words_str)\n",
    "\n",
    "\n",
    "    # build vocab\n",
    "    word_freq = {}\n",
    "    word_set = set()\n",
    "    for doc_words in shuffle_doc_words_list:\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    vocab = list(word_set)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_doc_list = {}\n",
    "\n",
    "    for i in range(len(shuffle_doc_words_list)):\n",
    "        doc_words = shuffle_doc_words_list[i]\n",
    "        words = doc_words.split()\n",
    "        appeared = set()\n",
    "        for word in words:\n",
    "            if word in appeared:\n",
    "                continue\n",
    "            if word in word_doc_list:\n",
    "                doc_list = word_doc_list[word]\n",
    "                doc_list.append(i)\n",
    "                word_doc_list[word] = doc_list\n",
    "            else:\n",
    "                word_doc_list[word] = [i]\n",
    "            appeared.add(word)\n",
    "\n",
    "    word_doc_freq = {}\n",
    "    for word, doc_list in word_doc_list.items():\n",
    "        word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "    word_id_map = {}\n",
    "    for i in range(vocab_size):\n",
    "        word_id_map[vocab[i]] = i\n",
    "\n",
    "    vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "    with open('data/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
    "        f.write(vocab_str)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Word definitions begin\n",
    "    '''\n",
    "    '''\n",
    "    definitions = []\n",
    "\n",
    "    for word in vocab:\n",
    "        word = word.strip()\n",
    "        synsets = wn.synsets(clean_str(word))\n",
    "        word_defs = []\n",
    "        for synset in synsets:\n",
    "            syn_def = synset.definition()\n",
    "            word_defs.append(syn_def)\n",
    "        word_des = ' '.join(word_defs)\n",
    "        if word_des == '':\n",
    "            word_des = '<PAD>'\n",
    "        definitions.append(word_des)\n",
    "\n",
    "    string = '\\n'.join(definitions)\n",
    "\n",
    "\n",
    "    f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
    "    f.write(string)\n",
    "    f.close()\n",
    "\n",
    "    tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "    tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
    "    tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "    print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n",
    "\n",
    "    word_vectors = []\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        vector = tfidf_matrix_array[i]\n",
    "        str_vector = []\n",
    "        for j in range(len(vector)):\n",
    "            str_vector.append(str(vector[j]))\n",
    "        temp = ' '.join(str_vector)\n",
    "        word_vector = word + ' ' + temp\n",
    "        word_vectors.append(word_vector)\n",
    "\n",
    "    string = '\\n'.join(word_vectors)\n",
    "\n",
    "    f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
    "    f.write(string)\n",
    "    f.close()\n",
    "\n",
    "    word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "    _, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "    word_embeddings_dim = len(embd[0])\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Word definitions end\n",
    "    '''\n",
    "\n",
    "    # label list\n",
    "    label_set = set()\n",
    "    for doc_meta in shuffle_doc_name_list:\n",
    "        temp = doc_meta.split('\\t')\n",
    "        label_set.add(temp[2])\n",
    "    label_list = list(label_set)\n",
    "\n",
    "    label_list_str = '\\n'.join(label_list)\n",
    "    with open('data/datax/' + dataset + '_labels.txt', 'w') as f:\n",
    "        f.write(label_list_str)\n",
    "\n",
    "\n",
    "    # x: feature vectors of training docs, no initial features\n",
    "    # slect 90% training set\n",
    "    train_size = len(train_ids)\n",
    "    val_size = int(0.1 * train_size)\n",
    "    real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "    # different training rates\n",
    "\n",
    "    real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "    real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "    with open('data/datax/' + dataset + '.real_train.name', 'w') as f:\n",
    "        f.write(real_train_doc_names_str)\n",
    "\n",
    "\n",
    "    row_x = []\n",
    "    col_x = []\n",
    "    data_x = []\n",
    "    for i in range(real_train_size):\n",
    "        doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "        doc_words = shuffle_doc_words_list[i]\n",
    "        words = doc_words.split()\n",
    "        doc_len = len(words)\n",
    "        for word in words:\n",
    "            if word in word_vector_map:\n",
    "                word_vector = word_vector_map[word]\n",
    "                # print(doc_vec)\n",
    "                # print(np.array(word_vector))\n",
    "                doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "        for j in range(word_embeddings_dim):\n",
    "            row_x.append(i)\n",
    "            col_x.append(j)\n",
    "            # np.random.uniform(-0.25, 0.25)\n",
    "            data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "    # x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "    x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "        real_train_size, word_embeddings_dim))\n",
    "\n",
    "    y = []\n",
    "    for i in range(real_train_size):\n",
    "        doc_meta = shuffle_doc_name_list[i]\n",
    "        temp = doc_meta.split('\\t')\n",
    "        label = temp[2]\n",
    "        one_hot = [0 for l in range(len(label_list))]\n",
    "        label_index = label_list.index(label)\n",
    "        one_hot[label_index] = 1\n",
    "        y.append(one_hot)\n",
    "    y = np.array(y)\n",
    "    #print(\"XX\",x,y)\n",
    "\n",
    "    # tx: feature vectors of test docs, no initial features\n",
    "    test_size = len(test_ids)\n",
    "\n",
    "    row_tx = []\n",
    "    col_tx = []\n",
    "    data_tx = []\n",
    "    for i in range(test_size):\n",
    "        doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "        doc_words = shuffle_doc_words_list[i + train_size]\n",
    "        words = doc_words.split()\n",
    "        doc_len = len(words)\n",
    "        for word in words:\n",
    "            if word in word_vector_map:\n",
    "                word_vector = word_vector_map[word]\n",
    "                doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "        for j in range(word_embeddings_dim):\n",
    "            row_tx.append(i)\n",
    "            col_tx.append(j)\n",
    "            # np.random.uniform(-0.25, 0.25)\n",
    "            data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "    # tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "    tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                       shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "    ty = []\n",
    "    for i in range(test_size):\n",
    "        doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "        temp = doc_meta.split('\\t')\n",
    "        label = temp[2]\n",
    "        one_hot = [0 for l in range(len(label_list))]\n",
    "        label_index = label_list.index(label)\n",
    "        one_hot[label_index] = 1\n",
    "        ty.append(one_hot)\n",
    "    ty = np.array(ty)\n",
    "    #print(ty)\n",
    "\n",
    "    # allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "    # (a superset of x)\n",
    "    # unlabeled training instances -> words\n",
    "\n",
    "    word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                     (vocab_size, word_embeddings_dim))\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        if word in word_vector_map:\n",
    "            vector = word_vector_map[word]\n",
    "            word_vectors[i] = vector\n",
    "\n",
    "    row_allx = []\n",
    "    col_allx = []\n",
    "    data_allx = []\n",
    "\n",
    "    for i in range(train_size):\n",
    "        doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "        doc_words = shuffle_doc_words_list[i]\n",
    "        words = doc_words.split()\n",
    "        doc_len = len(words)\n",
    "        for word in words:\n",
    "            if word in word_vector_map:\n",
    "                word_vector = word_vector_map[word]\n",
    "                doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "        for j in range(word_embeddings_dim):\n",
    "            row_allx.append(int(i))\n",
    "            col_allx.append(j)\n",
    "            # np.random.uniform(-0.25, 0.25)\n",
    "            data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(word_embeddings_dim):\n",
    "            row_allx.append(int(i + train_size))\n",
    "            col_allx.append(j)\n",
    "            data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "    row_allx = np.array(row_allx)\n",
    "    col_allx = np.array(col_allx)\n",
    "    data_allx = np.array(data_allx)\n",
    "\n",
    "    allx = sp.csr_matrix(\n",
    "        (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "    ally = []\n",
    "    for i in range(train_size):\n",
    "        doc_meta = shuffle_doc_name_list[i]\n",
    "        temp = doc_meta.split('\\t')\n",
    "        label = temp[2]\n",
    "        one_hot = [0 for l in range(len(label_list))]\n",
    "        label_index = label_list.index(label)\n",
    "        one_hot[label_index] = 1\n",
    "        ally.append(one_hot)\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        one_hot = [0 for l in range(len(label_list))]\n",
    "        ally.append(one_hot)\n",
    "\n",
    "    ally = np.array(ally)\n",
    "\n",
    "    print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "    '''\n",
    "    Doc word heterogeneous graph\n",
    "    '''\n",
    "\n",
    "    # word co-occurence with context windows\n",
    "    window_size = 20\n",
    "    windows = []\n",
    "\n",
    "    for doc_words in shuffle_doc_words_list:\n",
    "        words = doc_words.split()\n",
    "        length = len(words)\n",
    "        if length <= window_size:\n",
    "            windows.append(words)\n",
    "        else:\n",
    "            # print(length, length - window_size + 1)\n",
    "            for j in range(length - window_size + 1):\n",
    "                window = words[j: j + window_size]\n",
    "                windows.append(window)\n",
    "                # print(window)\n",
    "\n",
    "\n",
    "    word_window_freq = {}\n",
    "    for window in windows:\n",
    "        appeared = set()\n",
    "        for i in range(len(window)):\n",
    "            if window[i] in appeared:\n",
    "                continue\n",
    "            if window[i] in word_window_freq:\n",
    "                word_window_freq[window[i]] += 1\n",
    "            else:\n",
    "                word_window_freq[window[i]] = 1\n",
    "            appeared.add(window[i])\n",
    "\n",
    "    word_pair_count = {}\n",
    "    for window in windows:\n",
    "        for i in range(1, len(window)):\n",
    "            for j in range(0, i):\n",
    "                word_i = window[i]\n",
    "                word_i_id = word_id_map[word_i]\n",
    "                word_j = window[j]\n",
    "                word_j_id = word_id_map[word_j]\n",
    "                if word_i_id == word_j_id:\n",
    "                    continue\n",
    "                word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "                if word_pair_str in word_pair_count:\n",
    "                    word_pair_count[word_pair_str] += 1\n",
    "                else:\n",
    "                    word_pair_count[word_pair_str] = 1\n",
    "                # two orders\n",
    "                word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "                if word_pair_str in word_pair_count:\n",
    "                    word_pair_count[word_pair_str] += 1\n",
    "                else:\n",
    "                    word_pair_count[word_pair_str] = 1\n",
    "\n",
    "    row = []\n",
    "    col = []\n",
    "    weight = []\n",
    "\n",
    "    # pmi as weights\n",
    "\n",
    "    num_window = len(windows)\n",
    "\n",
    "    for key in word_pair_count:\n",
    "        temp = key.split(',')\n",
    "        i = int(temp[0])\n",
    "        j = int(temp[1])\n",
    "        count = word_pair_count[key]\n",
    "        word_freq_i = word_window_freq[vocab[i]]\n",
    "        word_freq_j = word_window_freq[vocab[j]]\n",
    "        pmi = log((1.0 * count / num_window) /\n",
    "                  (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "        if pmi <= 0:\n",
    "            continue\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "\n",
    "    # word vector cosine similarity as weights\n",
    "\n",
    "    '''\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "                vector_i = np.array(word_vector_map[vocab[i]])\n",
    "                vector_j = np.array(word_vector_map[vocab[j]])\n",
    "                similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "                if similarity > 0.9:\n",
    "                    print(vocab[i], vocab[j], similarity)\n",
    "                    row.append(train_size + i)\n",
    "                    col.append(train_size + j)\n",
    "                    weight.append(similarity)\n",
    "    '''\n",
    "    # doc word frequency\n",
    "    doc_word_freq = {}\n",
    "\n",
    "    for doc_id in range(len(shuffle_doc_words_list)):\n",
    "        doc_words = shuffle_doc_words_list[doc_id]\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_id = word_id_map[word]\n",
    "            doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "            if doc_word_str in doc_word_freq:\n",
    "                doc_word_freq[doc_word_str] += 1\n",
    "            else:\n",
    "                doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "    for i in range(len(shuffle_doc_words_list)):\n",
    "        doc_words = shuffle_doc_words_list[i]\n",
    "        words = doc_words.split()\n",
    "        doc_word_set = set()\n",
    "        for word in words:\n",
    "            if word in doc_word_set:\n",
    "                continue\n",
    "            j = word_id_map[word]\n",
    "            key = str(i) + ',' + str(j)\n",
    "            freq = doc_word_freq[key]\n",
    "            if i < train_size:\n",
    "                row.append(i)\n",
    "            else:\n",
    "                row.append(i + vocab_size)\n",
    "            col.append(train_size + j)\n",
    "            idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                      word_doc_freq[vocab[j]])\n",
    "            weight.append(freq * idf)\n",
    "            doc_word_set.add(word)\n",
    "\n",
    "    node_size = train_size + vocab_size + test_size\n",
    "    adj = sp.csr_matrix(\n",
    "        (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "    # dump objects\n",
    "\n",
    "    with open(\"data/datax/ind.{}.x\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(x, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.y\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(y, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.tx\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(tx, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.ty\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(ty, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.allx\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(allx, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.ally\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(ally, f)\n",
    "\n",
    "    with open(\"data/datax/ind.{}.adj\".format(dataset), 'wb') as f:\n",
    "        pkl.dump(adj, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\"\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "\n",
    "###############################################\n",
    "# This section of code adapted from tkipf/gcn #\n",
    "###############################################\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/datax/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "    train_idx_orig = parse_index_file(\"data/datax/{}.train.index\".format(dataset_str))\n",
    "    train_idx_range = np.arange(len(train_idx_orig))\n",
    "    test_idx_reorder = parse_index_file(\"data/datax/{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    #adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = train_idx_range.tolist()\n",
    "    idx_val = range(len(idx_train),len(y))\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "\n",
    "    #print(adj.shape)\n",
    "    #print(features.shape)\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_random_data(size):\n",
    "\n",
    "    adj = sp.random(size, size, density=0.002) # density similar to cora\n",
    "    features = sp.random(size, 1000, density=0.015)\n",
    "    int_labels = np.random.randint(7, size=(size))\n",
    "    labels = np.zeros((size, 7)) # Nx7\n",
    "    labels[np.arange(size), int_labels] = 1\n",
    "\n",
    "    train_mask = np.zeros((size,)).astype(bool)\n",
    "    train_mask[np.arange(size)[0:int(size/2)]] = 1\n",
    "\n",
    "    val_mask = np.zeros((size,)).astype(bool)\n",
    "    val_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    test_mask = np.zeros((size,)).astype(bool)\n",
    "    test_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "  \n",
    "    # sparse NxN, sparse NxF, norm NxC, ..., norm Nx1, ...\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def preprocess_adj_bias(adj):\n",
    "    num_nodes = adj.shape[0]\n",
    "    adj = adj + sp.eye(num_nodes)  # self-loop\n",
    "    adj[adj > 0.0] = 1.0\n",
    "    if not sp.isspmatrix_coo(adj):\n",
    "        adj = adj.tocoo()\n",
    "    adj = adj.astype(np.float32)\n",
    "    indices = np.vstack((adj.col, adj.row)).transpose()  # This is where I made a mistake, I used (adj.row, adj.col) instead\n",
    "    # return tf.SparseTensor(indices=indices, values=adj.data, dense_shape=adj.shape)\n",
    "    return indices, adj.data, adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: R52\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.0005\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 2\n",
      "nb. units per layer: [128, 32]\n",
      "nb. attention heads: [10, 10, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x7fe97c3647b8>\n",
      "model: <class '__main__.SpGAT'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:162: RuntimeWarning: divide by zero encountered in power\n"
     ]
    }
   ],
   "source": [
    "checkpt_file = 'R8/mod_R.ckpt'\n",
    "\n",
    "dataset = 'R52'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 100000\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.0005  # weight decay\n",
    "hid_units = [128,32] # numbers of hidden units per each attention head in each layer\n",
    "n_heads = [10,10,1] # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "# model = GAT\n",
    "model = SpGAT\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "sparse = True\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(dataset)\n",
    "features, spars = preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "features = features[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "if sparse:\n",
    "    biases = preprocess_adj_bias(adj)\n",
    "else:\n",
    "    adj = adj.todense()\n",
    "    adj = adj[np.newaxis]\n",
    "    biases = adj_to_bias(adj, [nb_nodes], nhood=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f5f42b76990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mftr_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n",
    "        if sparse:\n",
    "            #bias_idx = tf.placeholder(tf.int64)\n",
    "            #bias_val = tf.placeholder(tf.float32)\n",
    "            #bias_shape = tf.placeholder(tf.int64)\n",
    "            bias_in = tf.sparse_placeholder(dtype=tf.float32)\n",
    "        else:\n",
    "            bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity)\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            tr_step = 0\n",
    "            tr_size = features.shape[0]\n",
    "\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                if sparse:\n",
    "                    bbias = biases\n",
    "                    #print(bbias.shape)\n",
    "                else:\n",
    "                    bbias = biases[tr_step*batch_size:(tr_step+1)*batch_size]\n",
    "\n",
    "                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        bias_in: bbias,\n",
    "                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        is_train: True,\n",
    "                        attn_drop: 0.6, ffd_drop: 0.6})\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = features.shape[0]\n",
    "\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                if sparse:\n",
    "                    bbias = biases\n",
    "                else:\n",
    "                    bbias = biases[vl_step*batch_size:(vl_step+1)*batch_size]\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        bias_in: bbias,\n",
    "                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        is_train: False,\n",
    "                        attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "\n",
    "            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg/vl_step\n",
    "                    vlss_early_model = val_loss_avg/vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "\n",
    "        saver.restore(sess, checkpt_file)\n",
    "\n",
    "        ts_size = features.shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            if sparse:\n",
    "                bbias = biases\n",
    "            else:\n",
    "                bbias = biases[ts_step*batch_size:(ts_step+1)*batch_size]\n",
    "            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n",
    "                feed_dict={\n",
    "                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    bias_in: bbias,\n",
    "                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    is_train: False,\n",
    "                    attn_drop: 0.0, ffd_drop: 0.0})\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
